---
title: "TO414_Group_11_Project_4"
author: "Jack Perlmuter, Ross Coyne, Logan Sabella, Mason Wicklander, Antonio De Llergo"
date: "2/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Laying out the Problem
We have chosen a data set that highlights whether or not patients had strokes and other associated information. We intend to build a model that can accurately predict whether or not a patient will have a stroke based on the presence of certain dependent variables (such as gender, heart disease, bmi). The business implications of this are for physicians to be able to plug their patients' information into our model and be able to predict whether or not the patient is going to have a stroke. As a result, patients can be made aware of their potential risk of stroke and stroke deaths can be prevented.


# Importing the Data Set and Cleaning It
```{r}
stroke <- read.csv("healthcare-dataset-stroke-data.csv")

#Observing summary and structure of data to identify what we need to clean
summary(stroke)
str(stroke)

#We decided to set ID to NULL, as it is an irrelevant variable
stroke$id <- NULL

#Renamed column stroke in order to prevent confusion
stroke$had_stroke <- stroke$stroke
stroke$stroke <- NULL

#We set the following columns as factors instead of characters
stroke$gender <- as.factor(stroke$gender)
stroke$smoking_status <- as.factor(stroke$smoking_status)
stroke$ever_married <- as.factor(stroke$ever_married)
stroke$work_type <- as.factor(stroke$work_type)
stroke$Residence_type <- as.factor(stroke$Residence_type)

#We set bmi to be numeric, as it was a character to begin. Then, we removed all NAs and replaced them with the mean (difference between mean and median was negligible, so we decided to preserve the mean)
stroke$bmi <- as.numeric(stroke$bmi)
stroke$bmi <- ifelse(is.na(stroke$bmi), mean(stroke$bmi, na.rm = T),stroke$bmi)

#Observing summary and structure of the cleaned data
summary(stroke)
str(stroke)
```

# Understanding the Data / Splitting the Data into Train and Test
```{r}
table(stroke$had_stroke)
prop.table(table(stroke$had_stroke))

#We need to upsample our data because it is a 95:5 split right now and we want it closer to 50:50
library(caret)
set.seed(17)

strokeup =upSample(x = stroke[,-ncol(stroke)], y = as.factor(stroke$had_stroke))
strokeup$had_stroke = strokeup$Class
strokeup$Class = NULL




#Should be using "trainup" for train sets and "test" for test sets from now on

# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

strokeupmm <- as.data.frame(model.matrix(~.-1,strokeup))
str(strokeupmm)

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

strokeup_random <- strokeupmm[sample(nrow(strokeupmm)),]
# we are going to normalize everything 
strokeup_norm <- as.data.frame(lapply(strokeup_random, normalize))

```

# KNN Model
```{r}
library(class)

sqrt(nrow(trainup_norm))

strokeup_label <- strokeup_norm[,"had_stroke1"]

knn_pred <- knn(train=strokeup_norm,test=strokeup_norm,cl=strokeup_label,k=83)
confusionMatrix(as.factor(knn_pred),as.factor(strokeup_norm$had_stroke1),positive="1")
```



# Building an ANN Model
```{r}
library(neuralnet)

# ANN model 
stroke.ann <- neuralnet(formula = had_stroke1 ~ .,
                              data = strokeup_norm, hidden = 1, stepmax = 1e+06)

# Visualize the network topology
plot(stroke.ann)

# Create a confusion matrix to see model accuracy
ann_pred <- predict(stroke.ann,strokeup_norm,type = "response")
ann_pred <- ifelse(ann_pred > 0.5, 1, 0)
confusionMatrix(as.factor(ann_pred),as.factor(strokeup_norm$had_stroke1),positive = "1")

```

# Logistic Regression Model
```{r}
logit.model <- glm(had_stroke ~ ., data = strokeup, family = "binomial")
summary(logit.model)

newlogit.model <- glm(had_stroke ~ age + hypertension + avg_glucose_level, data = strokeup, family = "binomial")
summary(newlogit.model)

logit_pred <- predict(logit.model,strokeup,type="response")
logit_pred <- ifelse(logit_pred>0.5,1,0)
confusionMatrix(as.factor(logit_pred),strokeup$had_stroke,positive="1")
```

=======
# SVM
```{r}
library(kernlab)

stroke.svm = ksvm(had_stroke ~., data = strokeup, kernel = "laplacedot")
stroke.svm

svm_pred = predict(stroke.svm, strokeup,type="response")
confusionMatrix(svm_pred,strokeup$had_stroke,positive="1")
```

# Decision Tree
```{r}
#Build the simplest decision tree
library(C50)
stroke.dt <- C5.0(strokeup[-11], strokeup$had_stroke)

stroke.dt
summary(stroke.dt)
plot(stroke.dt)

#Using model to predict
dt_pred <- predict(stroke.dt,strokeup)
confusionMatrix(as.factor(dt_pred),strokeup$had_stroke,positive = "1")




```

# Stacked Model
```{r}

#Checking to ensure data has been split properly
table(trainup_norm$had_stroke)
prop.table(table(trainup_norm$had_stroke))

# Creating Stacked Model
stroke_combined <- data.frame(logit_pred,knn_pred,ann_pred,svm_pred,dt_pred,strokeup$had_stroke)
stroke_combined$had_stroke <- stroke_combined$strokeup.had_stroke
stroke_combined$strokeup.had_stroke <- NULL

#Splitting the data into test and train
index <- createDataPartition(stroke_combined$had_stroke,p=0.7,list=FALSE)
train <- stroke_combined[index,]
test <- stroke_combined[-index,]

#Creating a cost matrix
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2)
error_cost

#Building a new decision tree to make a stacked model
#Apply the cost matrix to the new decision tree

stroke.stacked.model <- C5.0(train[-6],as.factor(train$had_stroke),costs = error_cost)

summary(stroke.stacked.model)

plot(stroke.stacked.model)

stacked_pred <- predict(stroke.stacked.model,test)

confusionMatrix(stacked_pred,as.factor(test$had_stroke),positive = "1")

```

# Non-Up Sampled Decision Tree
```{r}
stroke.dt.noUpsample <- C5.0(stroke[-11],as.factor(stroke$had_stroke))

summary(stroke.dt.noUpsample)

noUpsample_pred <- predict(stroke.dt.noUpsample,stroke)
confusionMatrix(noUpsample_pred,as.factor(stroke$had_stroke))
```

