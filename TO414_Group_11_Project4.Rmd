---
title: "TO414_Group_11_Project_4"
author: "Jack Perlmuter, Ross Coyne, Logan Sabella, Mason Wicklander, Antonio De Llergo"
date: "2/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Laying out the Problem
We have chosen a data set that highlights whether or not patients had strokes and other associated information. We intend to build a model that can accurately predict whether or not a patient will have a stroke based on the presence of certain dependent variables (such as gender, heart disease, bmi). The business implications of this are for physicians to be able to plug their patients' information into our model and be able to predict whether or not the patient is going to have a stroke. As a result, patients can be made aware of their potential risk of stroke and stroke deaths can be prevented.


# Importing the Data Set and Cleaning It
```{r}
stroke <- read.csv("healthcare-dataset-stroke-data.csv")

#Observing summary and structure of data to identify what we need to clean
summary(stroke)
str(stroke)

#We decided to set ID to NULL, as it is an irrelevant variable
stroke$id <- NULL

#Renamed column stroke in order to prevent confusion
stroke$had_stroke <- stroke$stroke
stroke$stroke <- NULL

#We set the following columns as factors instead of characters
stroke$gender <- as.factor(stroke$gender)
stroke$smoking_status <- as.factor(stroke$smoking_status)
stroke$ever_married <- as.factor(stroke$ever_married)
stroke$work_type <- as.factor(stroke$work_type)
stroke$Residence_type <- as.factor(stroke$Residence_type)

#We set bmi to be numeric, as it was a character to begin. Then, we removed all NAs and replaced them with the mean (difference between mean and median was negligible, so we decided to preserve the mean)
stroke$bmi <- as.numeric(stroke$bmi)
stroke$bmi <- ifelse(is.na(stroke$bmi), mean(stroke$bmi, na.rm = T),stroke$bmi)

#Observing summary and structure of the cleaned data
summary(stroke)
str(stroke)
```

# Understanding the Data / Splitting the Data into Train and Test
```{r}
table(stroke$had_stroke)
prop.table(table(stroke$had_stroke))

#We need to upsample our data because it is a 95:5 split right now and we want it closer to 50:50
library(caret)
set.seed(17)

#Splitting the data into test and train
index <- createDataPartition(stroke$had_stroke,p=0.7,list=FALSE)
train <- stroke[index,]
test <- stroke[-index,]

#Training the data up
trainup <- upSample(x = train[,-ncol(train)],
                  y=as.factor(train$had_stroke))
trainup$had_stroke <- trainup$Class
trainup$Class <- NULL

#Observing the structure of the new trainup set. As seen, it is a 50:50 split now
table(trainup$had_stroke)

#Should be using "trainup" for train sets and "test" for test sets from now on
```

## Splitting Data Into Train and Test
```{r}
test_set = sample(1:nrow(stroke), round(nrow(stroke)*.3))

stroke_log_test = stroke[test_set,]
stroke_log_train = stroke[-test_set,]

# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

strokemm <- as.data.frame(model.matrix(~.-1,stroke))
str(strokemm)

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

stroke_random <- strokemm[sample(nrow(strokemm)),]
# we are going to normalize everything 
stroke_norm <- as.data.frame(lapply(stroke_random, normalize))

#Creating normalized test and train sets
stroke_train_norm <- stroke_norm[-test_set, ]
stroke_test_norm <- stroke_norm[test_set, ]
```

=======
# Linear Regression
```{r}
linear.model<-lm(had_stroke~.,data=stroke)

summary(linear.model)

plot(linear.model)

new.linear.model<-lm(had_stroke~age+hypertension+heart_disease+ever_married+work_type+avg_glucose_level,data=stroke)

summary(new.linear.model)
```



# Building an ANN Model
```{r}
library(neuralnet)


#My ANN model 
stroke.ann <- neuralnet(formula = had_stroke ~ .,
                              data = stroke_train_norm, hidden = 1, stepmax = 1e+06)

# Visualize the network topology
plot(stroke.ann)

# Create a confusion matrix to see model accuracy
stroke_ann_pred <- predict(stroke.ann,stroke_test_norm,type = "response")
stroke_ann_pred <- ifelse(stroke_ann_pred > 0.1, 1, 0)
confusionMatrix(as.factor(stroke_ann_pred),as.factor(stroke_test_norm$had_stroke),positive = "1")

```

# Logit
```{r}
logit.model <- glm(had_stroke ~ ., data = stroke, family = "binomial")
summary(logit.model)

newlogitmodel <- glm(had_stroke ~ age + hypertension + avg_glucose_level, data = stroke, family = "binomial")
summary(newlogitmodel)
```


# Probit
```{r}
probit.model <- glm(had_stroke ~ ., data = stroke, family = binomial(link="probit"))
summary(probit.model)

#We tried a number of interaction variables but none were significant
newprobitmodel <- glm(had_stroke ~ age + hypertension + work_type + avg_glucose_level, data = stroke, family = binomial(link="probit"))
summary(newprobitmodel)

```
The new probit model that only included the significant variables was the best logistic regression model, as it has the lowest AIC score. We tested different interaction variables, but none were significant. After creating te optimal regression model we are created a prediction model along with a confusion matrix displayed below.

```{r}
# Predictions and Confusion Matrix Logistic Regression Model
#Predicting Logistic Regression
stroke_probit_pred <- predict(newprobitmodel,stroke, type = "response")
stroke_probit_pred <- ifelse(stroke_probit_pred > 0.50,1,0)
stroke_probit_pred



#Confusion Matrix
confusionMatrix(as.factor(newprobitmodel),as.factor(newlogitmodel), positive = "1")
```

<<<<<<< HEAD

=======
# SVM
```{r}
library(kernlab)

stroke_classifier = ksvm(had_stroke ~., data = stroke_log_train, kernel = "vanilladot")
stroke_classifier

stroke_predictions = predict(stroke_classifier, stroke_log_test)

table(stroke_predictions, stroke_log_test$had_stroke)
agreement <- stroke_predictions == stroke_log_test$had_stroke
table(agreement)
prop.table(table(agreement))
```

# Decision Tree
```{r}
#Build the simplest decision tree
library(C50)
stroke_decision_tree <- C5.0(stroke_log_train[-11], as.factor(stroke_log_train$had_stroke))

stroke_decision_tree
summary(stroke_decision_tree)
plot(stroke_decision_tree)

#Using model to predict
stroke_dt_pred <- predict(stroke_decision_tree, stroke_log_test)
#Creating a confusion matrix
library(caret)
confusionMatrix(as.factor(stroke_dt_pred),as.factor(stroke_log_test$had_stroke),positive = "1")

table(stroke_dt_pred)
```

